{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning tips and tricks: Data augmentation and Transfer learning\n",
    "\n",
    "**What you will learn today**: You will learn how to improve the performance of a deep neural network by increasing the effective data available for training using data augmentation. You will also learn how to leverage large pretrained models to boost the performance of a downstream task using transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the previous episode...\n",
    "\n",
    "On the previous lab we explored different techniques to train neural networks and implemented them using `PyTorch`. Our experiments showed that choosing the right learning rate and architecture are fundamental to generalize on a complex dataset such as CIFAR10. We learned how modern optimizers work, and explored new architectural concepts such as batch normalization and skip connections.\n",
    "\n",
    "In this lab, we will continue this journey and investigate other techniques to improve performance of a neural network. In particular, we will see how one can exploit the flexibility of stochastic gradient descent to increase our effective data available, and how we can leverage large pretrained models to initialize our networks in a smart way.\n",
    "\n",
    "Specifically, we will talk about:\n",
    "* Data augmentation\n",
    "* Transfer learning\n",
    "\n",
    "So... let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we copy-paste the necessary code from the previous lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every model we want to create, we will create a new class that inherits `BasicModel` and implemements the `__init__` and `forward` functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "Data augmentation is a training technique which consists in transforming every batch of data shown to the model using some random operation which generates a new ''view'' of each sample that retains its semantic information. For example, in the context of image classification, the label of most objects remains the same if you mirror them horizontally. Therefore, a cheap way to increase your training data, is to ''augment'' each sample by introducing its mirrored counterpart.\n",
    "\n",
    "Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we load all the necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from training_utils import fit, predict, plot_loss, visualize_images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `PyTorch`, the data augmentation operations are included in the transformation pipeline of a dataset. You can find more details on the [official documentation](https://pytorch.org/vision/stable/transforms.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.RandomHorizontalFlip() # We want to randomly apply a random flip to every sample\n",
    "])\n",
    "\n",
    "# We do not want to augment the test data, so we need a different transform pipeline\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(), \n",
    "])\n",
    "\n",
    "# load the train dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data/', \n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=train_transform)\n",
    "\n",
    "# load the test dataset\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data/', \n",
    "    train=False, \n",
    "    download=True,\n",
    "    transform=test_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `PyTorch`, any `nn.Module` can be included in a transform pipeline. Every time you ask for a sample `x`, `PyTorch` calls `transform.forward(x)` before feeding it to the model. This means we can easily visualize the effect of data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im, _ = test_dataset[0]\n",
    "im = im[None, ...] # We need a leading batch dimension to feed to the model\n",
    "\n",
    "images_rot = torch.cat([T.RandomHorizontalFlip()(im) for _ in range(10)])\n",
    "\n",
    "grid = torchvision.utils.make_grid(images_rot, nrow=5, padding=2)\n",
    "# Just normalization for test data\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.imshow(grid.permute([1,2,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horizontal flips are a bit obvious. We can get more creative with data augmentation. However, bare in mind that some transformations might destroy important information of your data, so be careful when applying it.\n",
    "\n",
    "CIFAR10 is relatively easy, so the following transformations are enough to achieve good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomCrop(size=[32,32], padding=4)\n",
    "])\n",
    "\n",
    "# We do not want to augment the test data, so we need a different transform pipeline\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(), \n",
    "])\n",
    "\n",
    "# load the train dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data/', \n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=train_transform)\n",
    "\n",
    "# load the test dataset\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data/', \n",
    "    train=False, \n",
    "    download=True,\n",
    "    transform=test_transform)\n",
    "\n",
    "# we make the Batch_sizes smaller for computational reasons\n",
    "# we will later use larger models and memory might become an issue\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True, \n",
    "    num_workers=2)\n",
    "\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, \n",
    "    num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a **ResNet**! Since it is a standard model, it is already implemented in the `torchvision` library and we can use it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torchvision.models.resnet import resnet18\n",
    "\n",
    "model = resnet18(num_classes=10).to(DEVICE)\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# train the ResNet\n",
    "plot_loss(\n",
    "    fit(\n",
    "        model,\n",
    "        train_dataloader = train_dataloader,\n",
    "        optimizer = optimizer,\n",
    "        epochs = 30,\n",
    "        device = DEVICE\n",
    "    )\n",
    ")\n",
    "\n",
    "# predict with the trained model\n",
    "predict(\n",
    "        model,\n",
    "        test_dataloader = test_dataloader,\n",
    "        device = DEVICE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "\n",
    "In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest.\n",
    "\n",
    "Transfer learning refers to the concept of initializing a neural network using the weights learned on a different task before training. Surprisingly, if the pretraining dataset is large enough, but also semantically ''close'' to the downstream task, using transfer learning, instead of regular training from random weights can significantly boost performance. Intuitively, transfer learning allows to recycle the features learned with a lot of data on the pretraining task, and leverage them to classify a new dataset.\n",
    "\n",
    "Let's see this in practice. To that end, we will follow the [PyTorch tutorial on transfer learning](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html), which is an excellent resource to learn how to implement advanced techniques in deep learning. We will minimally adapt the code to fit our streamlined API from the previous labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download our data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "remote_url = 'https://download.pytorch.org/tutorial/hymenoptera_data.zip'\n",
    "\n",
    "local_file = './data/hymenoptera_data.zip'\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "data = requests.get(remote_url)\n",
    "\n",
    "# Save file data to local copy\n",
    "with open(local_file, 'wb')as file:\n",
    "    file.write(data.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and extract it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "  \n",
    "# opening the zip file in READ mode\n",
    "with ZipFile(local_file, 'r') as zip:\n",
    "    # extracting all the files\n",
    "    print('Extracting all the files now...')\n",
    "    zip.extractall('./data')\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can work with this data directly using `torchvision`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# The normalization values are the average and std of each channel, precomputed\n",
    "# on the training images\n",
    "train_transform = T.Compose([\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Just normalization for test data\n",
    "test_transform =  T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    # Should we add the same normalization for test as we did for train? Explain.\n",
    "    ...\n",
    "])\n",
    "\n",
    "\n",
    "data_dir = 'data/hymenoptera_data'\n",
    "train_dataset = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'), train_transform)\n",
    "test_dataset = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'val'), test_transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "train_size = len(train_dataset)\n",
    "test_size = len(test_dataset)\n",
    "class_names = train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize a few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_images(\n",
    "    train_dataloader,\n",
    "    mean=torch.tensor([0.485, 0.456, 0.406]),\n",
    "    std=torch.tensor([0.229, 0.224, 0.225]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two major transfer learning techniques depending on which parts of the network are updated using the new data:\n",
    "1. Finetuning the full network\n",
    "2. Finetuning only the last layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see how a randomly initialized model performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model is a resnet18. How many outputs should it have?\n",
    "model = resnet18(num_classes=2).to(DEVICE)\n",
    "\n",
    "# We normally use SGD to finetune a large model\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "plot_loss(\n",
    "    fit(\n",
    "        model,\n",
    "        train_dataloader = train_dataloader,\n",
    "        optimizer = optimizer,\n",
    "        epochs = 25,\n",
    "        device = DEVICE\n",
    "    )\n",
    ")\n",
    "\n",
    "# predict with the trained model\n",
    "predict(\n",
    "    model,\n",
    "    test_dataloader = test_dataloader,\n",
    "    device = DEVICE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning the full network\n",
    "Instead of random initialization, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. The rest of the training looks as usual, albeit normally using a significantly smaller learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FinetuningFullModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # load pretrained resnet18\n",
    "        self.network = ...\n",
    "        # change the last layer with a randonly initializied one and with `num_classes` number of output neurons.\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FinetuningFullModel(num_classes=...).to(DEVICE)\n",
    "\n",
    "# We normally use SGD to finetune a large model\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "plot_loss(\n",
    "    fit(\n",
    "        model,\n",
    "        train_dataloader = train_dataloader,\n",
    "        optimizer = optimizer,\n",
    "        epochs = 25,\n",
    "        device = DEVICE\n",
    "    )\n",
    ")\n",
    "\n",
    "# predict with the trained model\n",
    "predict(\n",
    "    model,\n",
    "    test_dataloader = test_dataloader,\n",
    "    device = DEVICE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning the last layer\n",
    "\n",
    "Sometimes finetuning all the parameters of a large model is too expensive, or unstable. In those cases, one can alternatively 'freeze' some parts of the network, and train only the latter parts. Most often, just tuning the last layer is enough to get good enough results, with the optimal performance normally achieved by finetuning a few of the last layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuningLastLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # load pretrained resnet18\n",
    "        self.network = ...\n",
    "\n",
    "        # freeze the parameters of the network.\n",
    "        ...\n",
    "\n",
    "        # replace the last layer with a randonly initializied one and with `num_classes` number of output neurons.\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FinetuningLastLayer(num_classes=...).to(DEVICE)\n",
    "\n",
    "# We normally use SGD to finetune a large model\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "plot_loss(\n",
    "    fit(\n",
    "        model,\n",
    "        train_dataloader = train_dataloader,\n",
    "        optimizer = optimizer,\n",
    "        epochs = 25,\n",
    "        device = DEVICE\n",
    "    )\n",
    ")\n",
    "\n",
    "# predict with the trained model\n",
    "predict(\n",
    "    model,\n",
    "    test_dataloader = test_dataloader,\n",
    "    device = DEVICE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
